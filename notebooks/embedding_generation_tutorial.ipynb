{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3abf8055",
   "metadata": {},
   "source": [
    "To execute this notebook, you need to either\n",
    " - Download a pre-trained model\n",
    " - Train an example model by excecuting the model_training_tutorial.ipynb notebook\n",
    "\n",
    "In this tutorial we will learn to:\n",
    "- Load a previously trained model\n",
    "- Extract DeepPrint features from fingerprint images\n",
    "- Evaluate the performance of the extracted fixed-length representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282c13c",
   "metadata": {},
   "source": [
    "## Embedding extraction\n",
    "\n",
    "After training the model, we can extract the DeepPrint features for the fingerprint images. This is done by calling the `extract` method of the `DeepPrintExtractor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f566250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from flx.extractor.fixed_length_extractor import get_DeepPrint_Tex, get_DeepPrint_TexMinu, DeepPrintExtractor\n",
    "\n",
    "# Dimension and number of training subjects must be known to load the pre-trained model\n",
    "\n",
    "# To load the pre-trained model parameters use num_training_subjects=8000\n",
    "extractor: DeepPrintExtractor = get_DeepPrint_TexMinu(num_training_subjects=10, num_dims=256)\n",
    "\n",
    "# To load the pre-trained model parameters use\n",
    "MODEL_DIR: str = os.path.abspath(\"example-model\") # Path to the directory containing the model parameters\n",
    "extractor.load_best_model(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33e2f8",
   "metadata": {},
   "source": [
    "Now we need to specify the dataset, for which we want to extract the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe661fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from flx.data.dataset import *\n",
    "from flx.data.image_loader import SFingeLoader\n",
    "from flx.data.transformed_image_loader import TransformedImageLoader\n",
    "from flx.image_processing.binarization import LazilyAllocatedBinarizer\n",
    "from flx.data.image_helpers import pad_and_resize_to_deepprint_input_size\n",
    "\n",
    "# NOTE: If this does not work, enter the absolute path to the notebooks/example-dataset directory here! \n",
    "DATASET_PATH: str = os.path.abspath(\"example-dataset\")\n",
    "\n",
    "# We will use the SFingeLoader to load the images from the dataset\n",
    "image_loader = TransformedImageLoader(\n",
    "        images=SFingeLoader(DATASET_PATH),\n",
    "        poses=None,\n",
    "        transforms=[\n",
    "            LazilyAllocatedBinarizer(5.0),\n",
    "            pad_and_resize_to_deepprint_input_size,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "image_dataset: Dataset = Dataset(image_loader, image_loader.ids)\n",
    "\n",
    "# The second value is for the minutiae branch, which we do not have in this example\n",
    "texture_embeddings, minutia_embeddings = extractor.extract(image_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f7d98",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "To evaluate the embeddings, we want to run a benchmark on them. For this, we must first specify the type of benchmark, and which comparisons should be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.scripts.generate_benchmarks import create_verification_benchmark\n",
    "\n",
    "NUM_IMPRESSIONS_PER_SUBJECT = 10\n",
    "benchmark = create_verification_benchmark(\n",
    "    subjects=list(range(image_dataset.num_subjects)),\n",
    "    impressions_per_subject=list(range(NUM_IMPRESSIONS_PER_SUBJECT))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37e9ef",
   "metadata": {},
   "source": [
    "Now we can run the benchmark. To do this, we must first specify the matcher (in our case cosine similarity of the embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.benchmarks.matchers import CosineSimilarityMatcher\n",
    "from flx.data.embedding_loader import EmbeddingLoader\n",
    "\n",
    "# We concatenate texture and minutia embedding vectors\n",
    "embeddings = EmbeddingLoader.combine(texture_embeddings, minutia_embeddings)\n",
    "matcher = CosineSimilarityMatcher(EmbeddingLoader.combine(texture_embeddings, minutia_embeddings))\n",
    "\n",
    "results = benchmark.run(matcher)\n",
    "\n",
    "print(f\"Equal-Error-Rate: {results.get_equal_error_rate()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee4287",
   "metadata": {},
   "source": [
    "To visualize the results, we can plot a DET curve. (Do not wonder if it is empty, probably the model is not trained enough. Take a look at the EER instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.visualization.plot_DET_curve import plot_verification_results\n",
    "\n",
    "figure_path = \"DET_curve\"\n",
    "\n",
    "# Lists are used to allow for multiple models to be plotted in the same figure\n",
    "plot_verification_results(figure_path, results=[results], model_labels=[\"DeepPrint_TexMinu\"], plot_title=\"example-dataset - verification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
