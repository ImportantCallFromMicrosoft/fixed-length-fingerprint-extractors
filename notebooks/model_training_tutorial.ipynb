{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f0c027",
   "metadata": {},
   "source": [
    "In this tutorial we will learn to:\n",
    "- Instantiate a DeepPrintExtractor\n",
    "- Train a DeepPrintExtractor\n",
    "- Extract DeepPrint features from fingerprint images\n",
    "- Evaluate the performance of the extracted fixed-length representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b5048",
   "metadata": {},
   "source": [
    "## Instantiate a DeepPrintExtractor\n",
    "\n",
    "This package implements a number of variants of the DeepPrint architecture. The wrapper class for all these variants is called `DeepPrintExtractor`.\n",
    "It has a `fit` method to train (and save) the model as well as an `extract` method to extract the DeepPrint features for fingerprint images. \n",
    "\n",
    "You can also try to implement your own models, but currently this is not directly supported by the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.data.dataset import IdentifierSet, Identifier\n",
    "from flx.extractor.fixed_length_extractor import get_DeepPrint_Tex, DeepPrintExtractor\n",
    "\n",
    "# We will use the example dataset with 10 subjects and 10 impression per subject\n",
    "training_ids: IdentifierSet = IdentifierSet([Identifier(i, j) for i in range(10) for j in range(10)])\n",
    "\n",
    "# We choose a dimension of 128 for the fixed-length representation\n",
    "extractor: DeepPrintExtractor = get_DeepPrint_Tex(num_training_subjects=training_ids.num_subjects, num_texture_dims=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9687ae",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Instantiating the model was easy. To train it, first we will load the training data (see the [data tutorial](./dataset_tutorial.ipynb) for how to implement your own dataset).\n",
    "\n",
    "Besides the fingerprint images, we also need a mapping from subjects to integer labels (for pytorch). For some variants we also need minutiae data. To see how a more complex dataset can be loaded, have a look at `flx/setup/datasets.py`.\n",
    "\n",
    "Finally, we call the `fit` method, which trains the model and saves it to the specified path.\n",
    "\n",
    "There is also the option to add a validation set, which will be used to evaluate the embeddings during training. This is useful to monitor the training progress and to avoid overfitting.\n",
    "In this example we will not use a validation set for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf461144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch \n",
    "\n",
    "from flx.data.dataset import *\n",
    "from flx.data.image_loader import SFingeLoader\n",
    "from flx.data.label_index import LabelIndex\n",
    "from flx.data.transformed_image_loader import TransformedImageLoader\n",
    "from flx.image_processing.binarization import LazilyAllocatedBinarizer\n",
    "from flx.data.image_helpers import pad_and_resize_to_deepprint_input_size\n",
    "\n",
    "# NOTE: If this does not work, enter the absolute path to the notebooks/example-dataset directory here! \n",
    "example_dataset_path = os.path.abspath(\"example-dataset\")\n",
    "outdir = os.path.join(os.path.dirname(example_dataset_path), \"output\")\n",
    "\n",
    "# We will use the SFingeLoader to load the images from the dataset\n",
    "image_loader = TransformedImageLoader(\n",
    "        images=SFingeLoader(example_dataset_path),\n",
    "        poses=None,\n",
    "        transforms=[\n",
    "            LazilyAllocatedBinarizer(5.0),\n",
    "            pad_and_resize_to_deepprint_input_size,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "image_dataset = Dataset(image_loader, training_ids)\n",
    "\n",
    "# For pytorch, we need to map the subjects to integer labels from [0 ... num_subjects-1]\n",
    "label_dataset = Dataset(LabelIndex(training_ids), training_ids)\n",
    "\n",
    "model_outdir = os.path.join(outdir, \"training\")\n",
    "extractor.fit(\n",
    "    fingerprints=image_dataset,\n",
    "    minutia_maps=None,\n",
    "    labels=label_dataset,\n",
    "    validation_fingerprints=None,\n",
    "    validation_benchmark=None,\n",
    "    num_epochs=20,\n",
    "    out_dir=model_outdir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f486151",
   "metadata": {},
   "source": [
    "## Embedding extraction\n",
    "\n",
    "After training the model, we can extract the DeepPrint features for the fingerprint images. This is done by calling the `extract` method of the `DeepPrintExtractor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410315c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the best model, use the following line: But assuming you just trained it, it should already be loaded\n",
    "# extractor.load_best_model(model_outdir)\n",
    "\n",
    "# The second value is for the minutiae branch, which we do not have in this example\n",
    "texture_embeddings, _ = extractor.extract(image_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cae0bf",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "To evaluate the embeddings, we want to run a benchmark on them. For this, we must first specify the type of benchmark, and which comparisons should be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d58904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.scripts.generate_benchmarks import create_verification_benchmark\n",
    "\n",
    "benchmark = create_verification_benchmark(subjects=list(range(10)), impressions_per_subject=list(range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e01307c",
   "metadata": {},
   "source": [
    "Now we can run the benchmark. To do this, we must first specify the matcher (in our case cosine similarity of the embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.benchmarks.matchers import CosineSimilarityMatcher\n",
    "\n",
    "matcher = CosineSimilarityMatcher(texture_embeddings)\n",
    "\n",
    "results = benchmark.run(matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402ea8a",
   "metadata": {},
   "source": [
    "To visualize the results, we can plot a DET curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b5dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.visualization.plot_DET_curve import plot_verification_results\n",
    "\n",
    "figure_path = os.path.join(outdir, \"DET_curve.png\")\n",
    "\n",
    "# Lists are used to allow for multiple models to be plotted in the same figure\n",
    "plot_verification_results(figure_path, results=[results], model_labels=[\"DeepPrint_Tex\"], plot_title=\"example-dataset - verification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7792d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biometrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
