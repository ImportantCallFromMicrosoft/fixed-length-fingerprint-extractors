{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e3a47c-4bb8-4046-a094-4f9a6c57b8be",
   "metadata": {},
   "source": [
    "In this tutorial you will learn:\n",
    " - How the `Identifier` class is used to manage biomteric datasets\n",
    " - How to work with the `DataLoader` and `Dataset` classes \n",
    " - How to load data from a directory of images and assign them identifiers\n",
    " - How to preprocess the data for training a DeepFinger model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d42588-1f63-4ba3-bccc-cfdddd28df51",
   "metadata": {},
   "source": [
    "## Identifier, DataLoader and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba76984",
   "metadata": {},
   "source": [
    "In our codebase, an `Identifier` has a `subject` (to differentiate distinct fingers) and an `impression` (to differentiate the impressions / samples taken from one finger).\n",
    "\n",
    "Although one person can have multiple fingers, we will treat each finger as a separate subject (class) for training the DeepFinger model. This has the advantage, that the model can be trained with ten times as many classes as if we would treat each person as a class. Additionally, some datasets may not provide all ten fingers for each person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6c439-62de-4c60-ab8b-9cc6381f9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.data.dataset import Identifier\n",
    "\n",
    "# Create an identifier for the first subject (first distinct finger, may belong to the same person as subject 1, 2, 3, ... 9) and second impression\n",
    "# Note that we start counting from 0. This has practical reasons during training.\n",
    "myid = Identifier(subject=0, impression=1)\n",
    "print(myid)\n",
    "\n",
    "assert myid == Identifier(0, 1) # With positional arguments\n",
    "assert myid != Identifier(0, 3) # Equals only if both subject and impression match\n",
    "assert myid != Identifier(1, 1) # Equals only if both subject and impression match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e1009-4d6c-4185-93a6-abe2af10cdcb",
   "metadata": {},
   "source": [
    "To manage multiple identifiers, we use an `IdentifierSet`. This data structure has many useful functions for filtering the contained `Identifier`s and also ensures their uniqueness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8296863-4a02-44ae-af50-c5e026d9747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.data.dataset import IdentifierSet\n",
    "\n",
    "id_set = IdentifierSet(\n",
    "    [\n",
    "        Identifier(0, 1),\n",
    "        Identifier(3, 1),\n",
    "        Identifier(5, 2),\n",
    "        Identifier(3, 0),\n",
    "        Identifier(1, 2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Total number of identifiers\n",
    "print(len(id_set))\n",
    "\n",
    "# Count how many different subjects\n",
    "print(id_set.num_subjects)\n",
    "\n",
    "# We can access the identifiers by index. Note that they are always sorted by (subject, impression)\n",
    "print(id_set[0])\n",
    "print(id_set[1])\n",
    "print(id_set[2])\n",
    "print(id_set[3])\n",
    "print(id_set[4])\n",
    "\n",
    "# We can filter the Indentifier set according to these indices:\n",
    "id_set_1to3 = id_set.filter_by_index([1, 2, 3])\n",
    "\n",
    "# We can also check if one set is a subset of another (useful to check if some dataset is complete)\n",
    "print(f\"id_set is superset of id_set_1to3: {id_set >= id_set_1to3}\")\n",
    "print(f\"id_set is subset of id_set_1to3: {id_set <= id_set_1to3}\")\n",
    "\n",
    "# identifiers can also be used as keys of a dictionary\n",
    "letter_dict = {id:letter for id, letter in zip(id_set, \"abcde\")}\n",
    "\n",
    "print(\"Letter for myid: \" + letter_dict[myid])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b474fb0-76ea-4f73-955d-8c28dfdab175",
   "metadata": {},
   "source": [
    "Now that we are familiar with how `Identifiers` work, understanding `DataLoader`s and `Dataset`s is not difficult.\n",
    "\n",
    "To implement your own `DataLoader` class you just need to derive from the abstract `DataLoader` class and then implement the `get` classmethod. This method takes a single identifier as input and returns the loaded value. There is no constraint on what can be loaded and in the `flx` codebase there are loaders for fingerprint, minutia maps, embedding vectors and other types of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229a30e-4953-40ab-a3ce-91ea8bab36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from flx.data.dataset import DataLoader\n",
    "\n",
    "class StringLoader(DataLoader):\n",
    "    def __init__(self, value_dict: dict[Identifier, str]):\n",
    "        self.value_dict = value_dict\n",
    "\n",
    "    def get(self, identifier: Identifier) -> str:\n",
    "        return self.value_dict[identifier]\n",
    "\n",
    "string_loader = StringLoader(letter_dict)\n",
    "print(string_loader.get(myid))\n",
    "\n",
    "class RandomNumberLoader(DataLoader):\n",
    "    def get(self, identifier: Identifier) -> float:\n",
    "        return random.random()\n",
    "\n",
    "random_number_loader = RandomNumberLoader()\n",
    "print(random_number_loader.get(myid))\n",
    "print(random_number_loader.get(myid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310751c-cc9c-4276-bab3-68ff5407ea1f",
   "metadata": {},
   "source": [
    "A `Dataset` is just a combination of an `IdentifierSet` and a `DataLoader`. Basically we say what is in the dataset through the `IdentifierSet` and how to access the actual values through the `DataLoader`. An advantage of this separation is, that we can easily create subsets and combine datasets, regardless of what kind of values we have in our `DataLoader`. We can even \"zip\" multiple datasets together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93dc3cf-f5aa-45bd-99dd-bebb60f04071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.data.dataset import Dataset\n",
    "\n",
    "string_dataset = Dataset(string_loader, id_set)\n",
    "\n",
    "# Datasets also have a get method\n",
    "assert string_dataset.get(myid) == string_loader.get(myid)\n",
    "\n",
    "# But they allow indexed access and iteration as well!\n",
    "for i, s in enumerate(string_dataset):\n",
    "    print(f\"{i}: {s}\")\n",
    "\n",
    "# We get the IdentifierSet via the \"ids\" property\n",
    "print(string_dataset.ids)\n",
    "\n",
    "\n",
    "# Now we see how we can zip datasets\n",
    "random_number_dataset = Dataset(random_number_loader, id_set)\n",
    "for tpl in Dataset.zip(string_dataset, random_number_dataset):\n",
    "    print(tpl)\n",
    "\n",
    "# And how to concatenate them\n",
    "some_other_ids = IdentifierSet([Identifier(i, 0) for i in range(10)])\n",
    "random_number_dataset2 = Dataset(random_number_loader, some_other_ids)\n",
    "\n",
    "# Here the subjects are shared, which means that subject 1 in dataset 1 is the same as subject 1 in dataset 2.\n",
    "# The Identifiers in the new dataset will therefore have the same subject but maybe a different impression\n",
    "# (as the same impression can in multiple datasets)\n",
    "shared_subjects = Dataset.concatenate(random_number_dataset, random_number_dataset2, share_subjects = True)\n",
    "for id in shared_subjects.ids:\n",
    "    print(id)\n",
    "\n",
    "# Here the subjects are not shared, which means that subject 1 in dataset 1 is a different subjects as subject 1 in dataset 2.\n",
    "# The Identifiers in the new dataset will therefore have the same impression but maybe a different subject\n",
    "# (as the same subject can appear in multiple datasets)\n",
    "separate_subjects = Dataset.concatenate(random_number_dataset, random_number_dataset2, share_subjects = False)\n",
    "for id in separate_subjects.ids:\n",
    "    print(id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041560c-d4d4-45c3-b6ea-2b6629ca6c57",
   "metadata": {},
   "source": [
    "## Loading an image dataset from disk\n",
    "\n",
    "For loading image datasets from disk there exists the `ImageLoader` class. It does most of the work of loading and indexing the image files inside a root dir (and its subdirectories).\n",
    "\n",
    "Take a look at the following folder structure, containing fingerprint images and fingerprint iso templates (.ist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87424353-ef31-4747-ac5f-15a678de5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "\n",
    "# NOTE: If this does not work, enter the absolute path to the notebooks/example-dataset directory here! \n",
    "example_dataset_path = os.path.abspath(\"example-dataset\")\n",
    "list_files(example_dataset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dae22c-fd52-4ce6-bb52-71a3ee4ea8e8",
   "metadata": {},
   "source": [
    "As one can see, the folder structure is rather complicated, however, the subject and impression can be read from the filename directly. We can use this to index and access the whole directory with just a few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28e564-1687-4c40-b077-d2c8f438e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.data.image_loader import ImageLoader\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as VTF\n",
    "import PIL\n",
    "from IPython.display import display\n",
    "\n",
    "# Derive from ImageLoader and implement the following three staticmethods:\n",
    "class ExampleImageLoader(ImageLoader):\n",
    "    # We do not need to override the __init__ method. In case you need to do it, call\n",
    "    # super().__init__(<my_root_dir>)\n",
    "    # with the root dir of the image dataset inside your __init__ method.\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extension() -> str:\n",
    "        return \".png\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _file_to_id_fun(subdir: str, filename: str) -> Identifier:\n",
    "        # We can ignore the subdir\n",
    "        # But the filename has the pattern: <subject>_<impression>.png\n",
    "        subject_id, impression_id = filename.split(\"_\")\n",
    "        return Identifier(int(subject_id), int(impression_id))\n",
    "        \n",
    "    @staticmethod\n",
    "    def _load_image(filepath: str) -> torch.Tensor:\n",
    "        img = PIL.Image.open(filepath)\n",
    "        img = PIL.ImageOps.grayscale(img)\n",
    "        # TODO: Resize / crop your image to the input size of DeepPrint (which is 299 x 299)#\n",
    "        # Take a look at flx.data.image_loader to see how this can be done.\n",
    "        \n",
    "        # To be compatible with DeepPrint, we convert it to pytorch.Tensor\n",
    "        # The image is now in format 1 x height x width and pixel values are scaled from [0. 255] to [0. 1]\n",
    "        return VTF.to_tensor(img)\n",
    "        \n",
    "image_loader = ExampleImageLoader(example_dataset_path)\n",
    "expected_ids = IdentifierSet([Identifier(i, j) for i in range(1, 11) for j in range(1, 11)])\n",
    "assert image_loader.ids == expected_ids # Not all DataLoaders have an identifier set, but ImageLoaders always have\n",
    "\n",
    "image_dataset = Dataset(image_loader, expected_ids)\n",
    "\n",
    "img = image_dataset[0]\n",
    "print(img.shape)\n",
    "display(VTF.to_pil_image(img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8428691-5ce0-40f4-9978-731536cf07f4",
   "metadata": {},
   "source": [
    "## Preprocessing images\n",
    "\n",
    "You can preprocess and augment your image using the `TransformedImageLoader` class. It accepts pose transformation (shift, rotation) which are applied first, followed by any number of image processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d913e3-37a0-4bb5-b1ab-c9a0c08a76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.image_processing.augmentation import RandomPoseTransform\n",
    "from flx.image_processing.binarization import LazilyAllocatedBinarizer\n",
    "from flx.data.transformed_image_loader import TransformedImageLoader\n",
    "\n",
    "transformed_images = TransformedImageLoader(\n",
    "    image_loader, # Just the regular image loader\n",
    "    poses = RandomPoseTransform(), # Augment the pose; Use this during training\n",
    "    transforms = [LazilyAllocatedBinarizer(ridge_width = 5.0)] # Apply Gabor Wavelets and binarize\n",
    ")\n",
    "\n",
    "image_dataset = Dataset(transformed_images, expected_ids)\n",
    "\n",
    "img = image_dataset[0]\n",
    "print(img.shape)\n",
    "display(VTF.to_pil_image(img))\n",
    "\n",
    "# Notice, that the pose transform adds padding. You may have to add a transform that crops / resizes it again to the correct size...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
